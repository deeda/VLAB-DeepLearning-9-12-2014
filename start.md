<h3>What is Deep Learning</h3>

Deep Learning is a new area of Machine Learning research, which has been introduced with the objective of moving Machine Learning closer to one of its original goals: Artificial Intelligence.

Deep Learning is about learning multiple levels of representation and abstraction that help to make sense of data such as images, sound, and text.

Deep learning is a set of algorithms in machine learning that attempt to model high-level abstractions in data by using model architectures composed of multiple non-linear transformations.

Deep learning is part of a broader family of machine learning methods based on learning representations of data. An observation (e.g., an image) can be represented in many ways (e.g., a vector of pixels), but some representations make it easier to learn tasks of interest (e.g., is this the image of a human face?) from examples, and research in this area attempts to define what makes better representations and how to create models to learn these representations.

Various deep learning architectures such as deep neural networks, convolutional deep neural networks, and deep belief networks have been applied to fields like computer vision, automatic speech recognition, natural language processing, and music/audio signal recognition where they have been shown to produce state-of-the-art results on various tasks.

Deep Learning involves iterative algorithms

It is different from the early Neural Network research in the 80s. Deep Learning involves learning using mulitple layers of abstraction and specifically, non-linear transforms.

<img src="http://i.imgur.com/P6rn9le.jpg">

========================

<h3>What's New in Deep Learning?</h3>

Deep Learning provides a methodology to make sense of all the data 
Internet gives you access to a lot of data, but there needs to be a sensible methodology to organize and create classification algorithms.

SLIDE: (Old School - data sources and prior methodologies)

Internet 
Metadata: tags, translations
Mechanical Turk

Deep Learning Involves:<br>

Truly Big Data (although deep learning classifiers have been trained with more accuracy using with smaller data sets)<br>
Algorithmic Advances <br>
(unlabeled layers) <br>
Synapse scale to huge numbers - mulitple parralel nodes and concurrent algorithms<br>

Comparison to get an idea:

<b>Neural Nets 1-10M SYNAPSES<br>
Google Brain 1B SYNAPSES<br>
Adult Brain - 100T SYNAPSES<br>
Infant 1 Quadrillion SYNAPSES<br></b>

Number of heartbeats the same across all mammals 
unlabeled data (just look at info and know its a cat for example)
unsupervised pre-training -> structure NN (feature detectors) 

110 years of Moore's Law
Humanities capacity to compute 
Intel not on curve for last 10 years - look to GPUs / NVidia 

Disrupting many industries 
Robot and AI 
post-CMOS era 
Manufacturing 
Synthetic Bio

Quantum Computing 
Training deep probabilistic model 

iterative algorithms generates amazing things that work or solve complex problems without always understanding why

Locus of learning shifts from product to process
Danny Hillis - two quotes - engineering 
